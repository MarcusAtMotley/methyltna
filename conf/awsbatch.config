/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    AWS Batch configuration for Motleybio-organization/methyltna
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Configuration for running the pipeline on AWS Batch.

    Prerequisites:
    - AWS Batch compute environment and job queue configured
    - S3 bucket for work directory and results
    - IAM roles with appropriate permissions
    - AWS CLI installed and configured

    Usage:
        nextflow run . -profile awsbatch \
            --input s3://bucket/samplesheet.csv \
            --outdir s3://bucket/results \
            -w s3://bucket/work
----------------------------------------------------------------------------------------
*/

params {
    config_profile_name        = 'AWS Batch profile'
    config_profile_description = 'Configuration for running on AWS Batch'

    // AWS-specific defaults
    // Users should override these via command line or config
    // aws_region              = 'us-east-1'
    // aws_queue               = 'your-batch-queue-name'
}

process {
    // Use AWS Batch executor
    executor = 'awsbatch'

    // Batch queue - MUST be set by user via --aws_queue parameter
    queue = { params.aws_queue ?: error("AWS Batch queue not specified. Use --aws_queue parameter") }

    // Container settings - use Docker (not Singularity)
    containerOptions = '--shm-size=16gb'  // Shared memory for some tools

    // Error handling
    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries    = 2
    maxErrors     = '-1'

    // Resource labels for AWS Batch
    // These map to different Batch compute environment settings
    withLabel: process_single {
        cpus   = 1
        memory = 4.GB
    }

    withLabel: process_low {
        cpus   = 2
        memory = 8.GB
    }

    withLabel: process_medium {
        cpus   = 8
        memory = 32.GB
    }

    withLabel: process_high {
        cpus   = 16
        memory = 64.GB
    }

    withLabel: process_long {
        cpus   = 4
        memory = 16.GB
        time   = 24.h
    }

    withLabel: process_high_memory {
        cpus   = 16
        memory = 128.GB
    }

    // Specific process overrides for AWS
    withName: 'STAR_ALIGN' {
        // STAR needs lots of memory for genome loading
        memory = { check_max( 64.GB * task.attempt, 'memory' ) }
    }

    withName: 'BISCUIT_ALIGN' {
        // Biscuit alignment can be memory intensive
        memory = { check_max( 32.GB * task.attempt, 'memory' ) }
    }

    withName: '.*:RSEQC_ANALYSIS:RSEQC_GENEBODYCOVERAGE' {
        // Even with subsampling, gene body coverage needs resources
        cpus   = 4
        memory = { check_max( 16.GB * task.attempt, 'memory' ) }
    }
}

aws {
    // AWS region - set via params.aws_region or environment variable
    region = params.aws_region ?: System.getenv('AWS_DEFAULT_REGION') ?: 'us-east-1'

    batch {
        // Path to AWS CLI (usually auto-detected)
        cliPath = '/usr/local/bin/aws'

        // Maximum parallel transfers to S3
        maxParallelTransfers = 4

        // Maximum transfer attempts
        maxTransferAttempts = 3

        // Delay between attempts
        delayBetweenAttempts = '5 sec'

        // Job role ARN (optional - can be set per compute environment)
        // jobRole = 'arn:aws:iam::ACCOUNT:role/BatchJobRole'
    }

    client {
        // Upload storage class for S3
        storageClass = 'STANDARD'

        // Connection and socket timeout
        maxConnections = 10
        connectionTimeout = 10000
        uploadStorageClass = 'INTELLIGENT_TIERING'

        // Upload settings for large files
        uploadMaxThreads = 4
        uploadChunkSize = '100MB'
        uploadMaxAttempts = 3

        // Download settings
        downloadMaxAttempts = 3
    }
}

// Increase time allowed for Batch to pull large container images
docker {
    registry = 'quay.io'
}

// S3 work directory is required
// Users must set via -w parameter: -w s3://bucket/work
// workDir should be set on command line or in user config

// Enable CloudWatch logging
trace {
    enabled = true
    overwrite = true
}

// Enable tower if using Seqera Platform
// tower {
//     enabled = true
//     endpoint = '-'
//     accessToken = params.tower_access_token
// }
