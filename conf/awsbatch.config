/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    AWS Batch configuration for Motleybio-organization/methyltna
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Configuration for running the pipeline on AWS Batch.

    Prerequisites:
    - AWS Batch compute environment and job queue configured
    - S3 bucket for work directory and results
    - IAM roles with appropriate permissions
    - AWS CLI installed and configured

    Usage:
        nextflow run . -profile awsbatch \
            --input s3://bucket/samplesheet.csv \
            --outdir s3://bucket/results \
            -w s3://bucket/work
----------------------------------------------------------------------------------------
*/

params {
    config_profile_name        = 'AWS Batch profile'
    config_profile_description = 'Configuration for running on AWS Batch'

    // AWS-specific defaults
    // Users should override these via command line or config
    // aws_region              = 'us-east-1'
    // aws_queue               = 'your-batch-queue-name'
}

process {
    // Use AWS Batch executor
    executor = 'awsbatch'

    // Batch queue - MUST be set by user via --aws_queue parameter
    queue = { params.aws_queue ?: error("AWS Batch queue not specified. Use --aws_queue parameter") }

    // Container settings - AWS Batch uses its own container configuration
    // Note: --shm-size is not directly supported; use process-level settings if needed

    // Error handling
    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries    = 2
    maxErrors     = '-1'

    // Resource labels for AWS Batch
    // These map to different Batch compute environment settings
    withLabel: process_single {
        cpus   = 1
        memory = 4.GB
    }

    withLabel: process_low {
        cpus   = 2
        memory = 8.GB
    }

    withLabel: process_medium {
        cpus   = 8
        memory = 32.GB
    }

    withLabel: process_high {
        // c5a.8xlarge has 32 vCPU and 64 GB - leave some headroom
        cpus   = 16
        memory = 60.GB
    }

    withLabel: process_long {
        cpus   = 4
        memory = 16.GB
        time   = 24.h
    }

    withLabel: process_high_memory {
        // Cap at instance max - c5a.8xlarge has 64 GB
        cpus   = 16
        memory = 60.GB
    }

    // Specific process overrides for AWS
    withName: 'STAR_ALIGN' {
        // STAR needs lots of memory for genome loading
        cpus   = 16
        memory = 60.GB
    }

    withName: 'BISCUIT_ALIGN' {
        // Biscuit alignment can be memory intensive
        cpus   = 8
        memory = 32.GB
    }

    withName: '.*:RSEQC_ANALYSIS:RSEQC_GENEBODYCOVERAGE' {
        // Even with subsampling, gene body coverage needs resources
        cpus   = 4
        memory = 16.GB
    }

    withName: '.*TRIMGALORE.*' {
        // Override process_high for TrimGalore - it doesn't need much
        cpus   = 4
        memory = 16.GB
    }

    withName: '.*FASTQC.*' {
        // FastQC is lightweight
        cpus   = 2
        memory = 8.GB
    }

    withName: '.*DOWNLOAD.*' {
        // Download tasks are I/O bound, not compute
        cpus   = 2
        memory = 4.GB
        // Use amazonlinux:2023 which has bash entrypoint and we install AWS CLI in script
        container = 'amazonlinux:2023'
    }

    // NOTE: Biocontainer entrypoint issues are handled by Wave containers
    // Wave automatically builds containers from conda environments with proper shell entrypoints
}

aws {
    // AWS region - set via params.aws_region or environment variable
    // Default to us-east-2 where motleybio infrastructure is located
    region = params.aws_region ?: System.getenv('AWS_DEFAULT_REGION') ?: 'us-east-2'

    batch {
        // Path to AWS CLI (usually auto-detected)
        cliPath = '/usr/local/bin/aws'

        // Maximum parallel transfers to S3
        maxParallelTransfers = 4

        // Maximum transfer attempts
        maxTransferAttempts = 3

        // Delay between attempts
        delayBetweenAttempts = '5 sec'

        // Job role ARN (optional - can be set per compute environment)
        // jobRole = 'arn:aws:iam::ACCOUNT:role/BatchJobRole'
    }

    client {
        // Upload storage class for S3
        storageClass = 'STANDARD'

        // Connection and socket timeout
        maxConnections = 10
        connectionTimeout = 10000
        uploadStorageClass = 'INTELLIGENT_TIERING'

        // Upload settings for large files
        uploadMaxThreads = 4
        uploadChunkSize = '100MB'
        uploadMaxAttempts = 3

        // Download settings
        downloadMaxAttempts = 3
    }
}

// AWS Batch uses Docker containers
docker {
    enabled = true
    registry = 'quay.io'
}

// Enable Wave containers for automatic entrypoint handling
// Wave builds containers from conda environments with proper shell entrypoints
// This solves the biocontainer entrypoint issue with AWS Batch
wave {
    enabled = true
    strategy = 'conda,container'
    freeze = true  // Wait for container build to complete before using
}

// S3 work directory is required
// Users must set via -w parameter: -w s3://bucket/work
// workDir should be set on command line or in user config

// Enable CloudWatch logging and report overwriting
trace {
    enabled = true
    overwrite = true
}

report {
    overwrite = true
}

timeline {
    overwrite = true
}

// Enable tower if using Seqera Platform
// tower {
//     enabled = true
//     endpoint = '-'
//     accessToken = params.tower_access_token
// }
